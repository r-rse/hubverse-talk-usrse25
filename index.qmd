---
title: "The Hubverse: Streamlining Collaborative Infectious Disease Modeling"
subtitle: "US-RSE Conference 2025"
author: 
  - name: Anna Krystalli 
    affiliation: 
      name: R-RSE SMPC
      url: r-rse.eu
    email: info@r-rse.eu
    orcid: 0000-0002-2378-4915
  - name: Consortium of Infectious Disease Modeling Hubs
    url: hubverse.io
date: "2025-10-07"
hex: logo.png
date-format: "D MMMM YYYY"
favicon: favicon.ico
format:
  revealjs:
    theme: [default, custom.scss, title.scss]
    template-partials:
      - title-slide.html
    mainfont: ['Poppins', sans-serif]
    lightbox: true
    fontsize: 1.5rem
    scrollable: true
    slide-number: true
    chalkboard: false
    notes: true
    logo: hubverse.png
    transition: fade
filters: [bg_style.lua]
execute:
  echo: true
params: 
  flusight_path: "~/Documents/workflows/UMASS/active-hubs/FluSight-forecast-hub"
---

# Background {.inverse}

## ‚ùå The problem

**Infectious disease modeling has scaled rapidly‚Ä¶**

-   But the landscape is fragmented:
    -   Inconsistent formats
    -   Redundant or conflicting forecasts
    -   Lack of coordination between modelers and stakeholders

> ‚ÄúComparing the accuracy of forecasting applications is difficult because forecasting methods, forecast outcomes, and reported validation metrics varied widely.‚Äù
>
> -   [Chretien et al., PLOS ONE, 2014](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0094130)

::: notes
Over the past two decades, and especially since the COVID-19 pandemic, **infectious disease modeling has become a central tool in public health decision-making**. We‚Äôve seen an explosion of forecasting efforts, model variations, and analytical platforms. This growth is **encouraging and reflects increased computational capacity**, improved data availability, and global interest in predictive analytics for public health.

However, this growth has come with growing pains. Forecasting efforts often develop independently, resulting in significant fragmentation across the ecosystem. This fragmentation limits the collective utility of modeling efforts.

-   Each group uses its own structure for forecasts, metadata, and evaluation, making integration and comparison difficult.

-   Modelers and decision-makers don‚Äôt always communicate clearly. Forecasts may not match what public health needs, or be interpretable in real time.

    In summary, the modeling field is rich, but messy. The challenge isn‚Äôt just making forecasts, it‚Äôs making them **usable**, **comparable**, and **coordinated**.
:::

------------------------------------------------------------------------

## ‚ú® The promise of modeling hubs

**Modeling hubs** coordinate collaborative forecasting:

-   Define standards and targets
-   Improve transparency and comparability
-   Aggregate forecasts enabling ensembles
-   Facilitate public health decision-making

::: notes
To address the fragmentation of infectious disease forecasting, modeling hubs have been proposed as a solution.

-   Hubs bring structure. But they aren‚Äôt just data repositories ,  they‚Äôre collaborative infrastructures.

-   They establish a shared forecasting protocol: what outcomes to forecast, for which locations and time horizons, in what format.

-   By enforcing standards and submission rules, hubs ensure that forecasts can be examined and compared.

-   Standardization also allows for ensemble models that combine many forecasts, often outperforming individual models ,  a major asset during uncertain conditions.

-   Hubs make it easier for stakeholders ,  from local agencies to national institutions ,  to access timely, interpretable forecasts.

-   Modeling hubs transform isolated efforts into coordinated forecasting ecosystems ,  laying the groundwork for both scientific insight and real-world action.
:::

------------------------------------------------------------------------

## üï∞Ô∏è Project origins {.smaller}

-   **Pre-COVID:** Forecasting code base existed for CDC **influenza** hubs
-   **During COVID:** That code was reused for new **COVID-19** hubs + demand internationally (e.g. Europe) for similar setups
-   ‚ùó Problem: Each hub required **manual editing** of source code

**‚û°Ô∏è Need for generalisation, modularity, and configurability**

![Figure credits: Alex Vespignani and Nicole Samay](assets/images/hub-history.png){fig-alt="Timeline of forecasting hub development" fig-align="left" width="700"}

::: notes
Before COVID, the US CDC had already established a foundation for infectious disease forecasting through its FluSight initiative, focused on influenza. ‚Ä¢ This was supported an initial working codebase.

During the COVID-19 pandemic, interest in modeling hubs exploded and this infrastructure was repurposed rapidly: ‚Ä¢ First by the US COVID-19 Forecast Hub ‚Ä¢ Then interest grew to scenario modeling hubs, and internationally.

But each of new hub required manual changes to the same core codebase.

This motivated the development of a more modular, configurable, and generalized system to support new hubs.
:::

------------------------------------------------------------------------

## üåê Enter the **hubverse**

An open-source **software ecosystem** to power modeling hubs:

-   Data standards for probabilistic forecast data
-   Schema-driven configuration for modeling tasks + hub setup
-   Modular tools for validation, evaluation, ensembling, and hub administration
-   Supports full lifecycle: from hub set up, data submission to decision-making

::: notes
The hubverse -which is an open-source software ecosystem designed specifically to support the full lifecycle of modeling hubs- was developed in response to the challenges of manual hub setup and maintenance.

It brings together modular, reusable tools that handle: ‚Ä¢ Standardised formats for probabilistic forecast data ,  improving interoperability ‚Ä¢ Schema-driven configuration, which defines modeling tasks, deadlines, and expectations. ‚Ä¢ A suite of tools for validation, evaluation, ensembling, and administration ,  all plug-and-play
:::

# Hubverse overview

------------------------------------------------------------------------

## ‚òëÔ∏è Standardised modeling tasks

Modeling hubs are built around a shared data standard:

-   **Modeling task definition**: targets (response variables), standard predictors, output types (e.g. `mean`, `quantiles`)
-   **Structured hub layout**: consistent file system for organizing submissions
-   **Standard model output format**: for file content and naming

‚úÖ Enables comparability, validation, and streamlined data access

::: notes
At its core, the hubverse represents an **expected data standard** for modeling hubs.\
Everything else builds on that foundation.

Each modeling task is defined up front ,  what‚Äôs being predicted, which predictors are to be used, and what output types are accepted.\
This removes ambiguity and ensures every team is working to the same specification.

Model outputs are submitted within a **structured hub layout**, so files are always stored in predictable locations.\
This makes them easy to locate, validate and access.

Finally, the **model output files themselves** follow a consistent format and naming convention.\
Together, these standards make model outputs directly comparable and easy to combine in downstream analysis.
:::

------------------------------------------------------------------------

## ‚öôÔ∏è Config-driven hub setup

Hub administrators configure hubs using structured JSON config files:

-   `admin.json`: hub-level metadata.
-   `task.json`: modeling task specification:
    -   **Task IDs**: Targets (response), horizons, locations (predictors) etc.
    -   **Output types**: accepted model outputs e.g. `mean`, `median`, `quantiles`, `cdf`, `pmf`, `samples`.
-   Configs are validated against a shared JSON [**schema**](https://github.com/hubverse-org/schemas)

::: notes
A key shift in the hubverse is that **hub setup is now configuration-based** ,  not hardcoded.

Administrators define how the hub works through **structured JSON config files**, making setup transparent, reproducible, and easy to audit.

-   `admin.json` describes hub-level properties like name, timezone, and schedule.\
-   `task.json` defines what models are expected to do: what targets (response variable), what predictor variables (other task IDs), and which output types are accepted for a given modeling task.

These files are validated against a shared **JSON schema**, which means we can build tooling around them:\
for example, validating submissions, generating dashboards, or other automated workflows.

The result is that new hubs can be spun up quickly, and modeling tasks are clearly and unambiguously defined from the start.

This enables clear definition and communication of tasks by administrators ensuring modeling efforts are policy relevant.
:::

------------------------------------------------------------------------

## üì¶ The R (and friends) package stack

The hubverse package ecosystem is organized by **role**. Each tool is designed to support a particular group of users in the hub workflow.

::::: columns
::: {.column width="40%"}
**Hub roles**

-   üõ†Ô∏è Hub administrators\
-   üî¨ Modelers\
-   üìä Analysts\
-   üèõÔ∏è Policy makers\
:::

::: {.column width="60%"}
**Tools & packages**

-   [`hubAdmin`](https://hubverse-org.r-universe.dev/hubAdmin) [{{< fa book >}}](https://hubverse-org.github.io/hubAdmin/): config creation + validation üõ†Ô∏è

-   [`hubValidations`](https://hubverse-org.r-universe.dev/hubValidations) [{{< fa book >}}](https://hubverse-org.github.io/hubValidations/): submission checks (structure, schema, content) üî¨ üõ†Ô∏è

-   [`hubData`](https://hubverse-org.r-universe.dev/hubData) (R) [{{< fa book >}}](https://hubverse-org.github.io/hubData/) / [`hubdata`](https://pypi.org/project/hubdata/) (Python) [{{< fa book >}}](https://hubverse-org.github.io/hub-data/): access multi-file model output via Arrow üõ†Ô∏è üî¨ üìä üèõÔ∏è

-   [`hubEvals`](https://hubverse-org.r-universe.dev/hubEvals) [{{< fa book >}}](https://hubverse-org.github.io/hubEvals/): compute evaluation metrics üõ†Ô∏è üìä üèõÔ∏è

-   [`hubEnsembles`](https://hubverse-org.r-universe.dev/hubEnsembles) [{{< fa book >}}](https://hubverse-org.github.io/hubEnsembles/): build weighted/unweighted ensembles üõ† üìä üèõÔ∏è

-   [`hubVis`](https://hubverse-org.r-universe.dev/hubVis) [{{< fa book >}}](https://hubverse-org.github.io/hubVis/): visualise model outputs üõ†Ô∏è üìä üèõÔ∏è
:::
:::::

All built for modularity, extensibility, and CI integration

::: notes
We've designed our R packages to match the different roles in a hub ,  from administrators to evaluators. Each tool handles a specific piece of the workflow.
:::

------------------------------------------------------------------------

## üìä Dashboards & communication

-   Built with [**Quarto**](https://quarto.org) so easily customisable via Quarto configuration
-   Deployed as a **fully static site** ,  no backend required
-   Powered by JSON data prepared via GitHub workflows
-   Interactive UI built with client-side JavaScript (fast!)
-   New instances can be set up by copying/configuring the [`hub-dashboard-template`](https://github.com/hubverse-org/hub-dashboard-template)

::: notes
Dashboards are crucial for communicating model outputs to a broader audience ,  including **policy makers, public health officials, and even the general public**.\
They help translate raw data and forecasts into accessible, actionable insights.

Our dashboard template is built in **Quarto**, so it‚Äôs easy to customise and extend.\
What makes it powerful is that it's **fully static** ,  no backend needed.\
JSON data are generated by **GitHub workflows**, and the JavaScript frontend renders it dynamically.

This design makes it fast, lightweight to host (e.g. on GitHub Pages), and simple to replicate.\
You can launch new dashboards quickly by copying and configuring the [`hub-dashboard-template`](https://github.com/hubverse-org/hub-dashboard-template).
:::

## ‚òÅÔ∏è Cloud hub storage and access

-   Hubs mirrored to public AWS S3 buckets
-   Forecast data available as Arrow datasets
-   Enables queryable data access via R üì¶ `hubData` and Python üì¶ `hub-data`.

::: notes
As a bonus, we mirror validated hub data to **AWS S3 buckets**, making it publicly accessible as **Arrow datasets**.

This enables downstream users ,  especially analysts ,  to query the data programmatically without needing to clone the full repository.\
They can access it via the R package `hubData` or the Python equivalent `hub-data`.

This **cloud mirroring** service is something the **hubverse currently provides**.\
To enable it for a new hub: - The hub administration needs to **contact us** to provision a bucket (we use **Pulumi** for infrastructure-as-code). - They simply need to add a bit of **lightweight configuration** to their `admin.json` file to activate it.
:::

------------------------------------------------------------------------

## üîÅ GitHub workflows

We automate everything we can:

-   ‚úÖ PR-level model output validation
-   ‚úÖ Hub configuration validation
-   ‚òÅÔ∏è AWS Cloud hub data synching
-   üìä dashboard data regeneration and model evaludation with each update

::: {.callout-note appearance="minimal" icon="false"}
All hubverse actions stored in the [**`hubverse-actions`**](https://github.com/hubverse-org/hubverse-actions) repo and can be installed with `hubCI::use_hub_github_action()`
:::

::: notes
**Automation is a cornerstone of the hubverse.**\
We use GitHub Actions to handle the full operational lifecycle of a hub.

This includes: - Validating all model output submissions at the **pull request** level, - Validating **hub configuration files**, - Synchronising validated model output to **cloud storage**, and - Regenerating dashboard data + computing model evaluation metrics automatically.

All workflows are modularised in the [`hubverse-actions`](https://github.com/hubverse-org/hubverse-actions) repo.\
They're easy to adopt ,  hub administrators can install them using the `hubCI::use_hub_github_action()` helper.

This approach ensures **reproducibility**, **consistency**, and **low overhead** for administrators.
:::

------------------------------------------------------------------------

## ü™© List of adopting hubs

{{< fa link >}} [**https://hubverse.io/community/hubs.html**](https://hubverse.io/community/hubs.html){.uri}

```{r}
#| echo: false
knitr::include_url("https://hubverse.io/community/hubs.html", height = "500px")
```

::: notes
Now that we've seen the infrastructure, let‚Äôs look at adoption.

This page on our site showcases all **hubs using the hubverse stack** ,  from long-standing hubs like the **US COVID-19 Forecast Hub** and the **European Scenario Hub**, to newer efforts tackling seasonal influenza and other diseases.

Each hub entry includes structured metadata: what disease it focuses on, when it was active, whether data are publicly available, and links to the relevant GitHub repos, dashboards and cloud storage.

This kind of **shared metadata and visibility** wouldn‚Äôt be possible without the consistency enforced by the hubverse framework.

It also serves as a **community resource** ,  a growing catalog of open modeling hubs that are easy to discover, learn from, or replicate.
:::

# ü¶† Real-world example: CDC FluSight

------------------------------------------------------------------------

## Real-world example: [CDC FluSight Hub](https://github.com/cdcepi/FluSight-forecast-hub)

{{< fa brands github >}} [**https://github.com/cdcepi/FluSight-forecast-hub**](https://github.com/cdcepi/FluSight-forecast-hub){.uri}

::::: columns
::: {.column width="70%"}
![](assets/images/flusight-hub.png){fig-alt="Screenshot of CDC Flusight Hub Github repo"} ![](assets/images/cdc-logo.png){width="100"}
:::

::: {.column .smaller width="30%"}
-   Used by US CDC to **monitor influenza severity**
-   **Weekly** forecasts from **40 teams** across **70 different models.**
-   Hosted on **GitHub + S3 cloud mirror**.
-   Managed using **full hubverse stack since 2023/2024 season**.
:::
:::::

::: notes
The CDC FluSight Hub is a flagship example of the hubverse stack in action.

It‚Äôs used by the **US CDC to monitor influenza severity** through **weekly model outputs** submitted by over **40 teams**, spanning **70 different models**.

The hub is fully managed using the hubverse software ,  including GitHub-based workflows and **S3 cloud mirroring** ,  since the **2023/24 season**.

If we peek into the repository structure, you'll see the key directories that define a hub:

-   `hub-config`: holds the JSON config files that describe the modeling tasks and hub metadata.
-   `model-output`: where teams submit their model output files
-   `target-data`: stores the observed data used to evaluate forecasts ,  such as reported influenza-like illness rates.
:::

------------------------------------------------------------------------

## üìÅ File structure: model output (CDC FluSight)

![](assets/images/flusight-model-outputs.png){fig-alt="Screenshot of flusight hub model output files"}

üóÉÔ∏è Forecasts are committed by model teams to versioned folders - One folder per team - One subfolder per forecast date - CSVs by location/region

::: notes
This slide breaks down a single model output file in the CDC FluSight hub.

On the left, we see the file lives under the `model-output/<model_id>` directory.

Files are named using a **round ID** and **model ID**, e.g., `2023-10-14-CADPH-FluCAT_Ensemble.csv`. Each file contains all predictions for that round by that model.

Inside the file: - The **Task ID columns** ,  like `target`, `horizon`, and `location` ,  define the specific modeling question. - The **Output Type columns** describe the prediction format (e.g., quantiles). - And the `value` column holds the actual predicted values.

This structure is fully standardised across all hubs and enables direct comparison, ensemble generation, and smooth downstream workflows.
:::

------------------------------------------------------------------------

## ‚úÖ Model output validation with [`hubValidations`](https://hubverse-org.r-universe.dev/hubValidations)

Model outputs submitted through PRs and validated through GitHub Actions

::::: columns
::: {.column width="40%"}
![](assets/images/flusight-pr.png){fig-alt="Screenshot of flusight hub model submission PRs"}
:::

::: {.column width="60%"}
![](assets/images/ga-action-screenshot.png){fig-alt="screenshot of flusight hub model submission validation results"}
:::
:::::

::: notes
Every model output is submitted via GitHub pull requests ,  and, once the appropriate hubverse workflow is set up- each submission is automatically validated using the `hubValidations` package.

On the left, we see recent submissions from different teams for a specific forecast round. Each corresponds to a PR.

On the right is an example validation log from GitHub Actions. Here, `hubValidations` checks that the submission: - Passes file level checks e.g.: - Follows the required file structure, - Is named and placed correctly, - And passes content-level checks: - valid task ID and output type value combinations, - required tasks submitted, - valid output types - output type specific checks (e.g quantile crossing) - Custom checks also supported

These automated checks catch errors early and keep the hub clean, and trustworthy ,  without manual intervention from administrators.
:::

------------------------------------------------------------------------

## üìÇ Accessing model output via [`hubData`](https://hubverse-org.r-universe.dev/hubData)

::::: columns
::: {.column width="35%"}
Connect to Arrow dataset of forecast submissions

```{r}
#| echo: false
run_if_local_hub_missing <- !dir.exists(params$flusight_path)
run_if_local_hub_exists <- dir.exists(params$flusight_path)
```

```{r}
#| eval: !expr run_if_local_hub_missing
library(hubData)

hub_path <- s3_bucket(
  "cdcepi-flusight-forecast-hub"
)
hub_con <- connect_hub(
  hub_path,
  skip_checks = TRUE
)
hub_con
```

```{r}
#| echo: false
#| eval: !expr run_if_local_hub_exists
library(hubData)
library(dplyr)

hub_path <- params$flusight_path
hub_con <- connect_hub(hub_path, skip_checks = TRUE)
hub_con
```
:::

::: {.column width="65%"}
Query and collect data

```{r}
# Filter for one model and forecast date using dplyr
library(dplyr)
hub_con |>
  filter(
    model_id == "CADPH-FluCAT_Ensemble",
    target_end_date == "2023-10-28"
  ) |>
  collect_hub()
```
:::
:::::

::: {.callout-tip appearance="minimal"}
See more in [Accessing data vignette](https://hubverse-org.github.io/hubData/articles/connect_hub.html).

Python analogue [**`hub-data`**](https://github.com/hubverse-org/hub-data) also available.
:::

::: notes
One of the big benefits of the hubverse format is that model outputs can be accessed as a queryable Arrow dataset.

Here we show how easy it is to connect to the FluSight hub's S3 mirror using `hubData`. No need to clone the repo ,  just connect directly via `s3_bucket()`.

The connection gives you a tidy, columnar dataset. You can then filter and collect data using standard `dplyr` verbs.

This makes it extremely easy for analysts, modelers, or public health teams to extract the exact subset of data they need.

The same functionality is available in Python via the `hub-data` package.
:::

------------------------------------------------------------------------

## üåê Ensembling with `hubEnsembles`

Combine models using simple or weighted rules

```{r}
forecast_df <- hub_con |>
  filter(
    model_id %in%
      c(
        "CADPH-FluCAT_Ensemble",
        "CEPH-Rtrend_fluH",
        "CFA_Pyrenew-Pyrenew_HE_Flu"
      ),
    output_type == "quantile"
  ) |>
  collect_hub()


hubEnsembles::simple_ensemble(forecast_df, model_id = "linear-pool-normal")
```

::: notes
Ensembling is central to the hub idea as they help improve prediction robustness by combining multiple model predictions into a single output. `hubEnsembles` provides a simple, pluggable interface to build median or weighted ensembles.

Once model outputs are collected we can ensemble them using `hubEnsembles`.

This example shows how to pull data from a few different models and apply a simple ensemble method called `"linear-pool-normal"`.

Since the hubverse format is standardised, all these operations can be applied across models without custom parsing.

And as always, this integrates cleanly with downstream workflows, CI pipelines, or dashboards.
:::

------------------------------------------------------------------------

## üìà Dashboard - [forecasts](https://reichlab.io/flusight-dashboard/forecast.html?as_of=2025-05-31&interval=95%25&target_var=wk+inc+flu+hosp&xaxis_range=2023-09-01&xaxis_range=2025-07-01&yaxis_range=-3787.6522689994536&yaxis_range=57169.8764352105&model=FluSight-ensemble&model=FluSight-baseline&location=US)

![](assets/images/dashboard-forecasts.png)

::: notes
This is the FluSight forecast dashboard, which is generated automatically from the model outputs and the observed target data we saw earlier.

It enables users to explore predictions interactively: you can choose outcomes, forecast dates, prediction intervals, and compare multiple models visually.

Crucially, the dashboard also shows the target (observed) data alongside forecasts, which helps contextualise model behaviour and monitor real-world performance.

Clicking on any model name in the left-hand panel opens that model‚Äôs metadata page ,  which includes details about the model, contributors, assumptions, and links to documentation or papers. This supports transparency and trust.

The dashboard itself is entirely static and regenerated automatically with each update to the hub. No backend server needed ,  just JSON + JavaScript.
:::

## ü©∫ Dashboard - [model evaluations](https://reichlab.io/flusight-dashboard/eval.html)

![](assets/images/dashboard-eval.png)

-   Evaluates forecasts against target (observed) data.

::: notes
This is the evaluation tab of the FluSight dashboard, which helps users compare model performance using standard metrics.

It‚Äôs powered by the `hubEvals` package, which computes evaluation scores by comparing model output to the target data stored in the hub.

The evaluation supports proper scoring rules such as: - **WIS** (Weighted Interval Score) - **MAE** (Mean Absolute Error) - **Coverage** of 50% and 95% prediction intervals

These are summarised into an interactive table, allowing for quick comparison across models.

Evaluations are automatically performed during the dashboard data build, so they always reflect the latest submissions.
:::

------------------------------------------------------------------------

## üí° Lessons & wider relevance

-   Standards + automation reduce friction
-   Open source keeps it accessible
-   Collaborative infrastructure empowers public health

::: notes
To wrap up:

-   Standardisation and automation reduce friction for everyone involved, whether submitting, validating, or analysing model outputs.
-   Open-source tools ensure transparency, reproducibility, and accessibility beyond just one institution or team.
-   And most importantly, this kind of collaborative infrastructure empowers public health efforts, by helping modelers and decision-makers work from a shared foundation of clean, consistent data.

We hope this inspires other communities to adopt or adapt similar workflows.
:::

------------------------------------------------------------------------

## üôè Thank you!

-   {{< fa link >}} <https://hubverse.io>
-   {{< fa book >}} <https://docs.hubverse.io>
-   {{< fa brands github >}} [`hubverse-org`](https://github.com/hubverse-org)
-   {{< fa envelope >}} [info\@r-rse.eu](mailto:info@r-rse.eu)

::: callout-tip
Interested in getting involved in the community? Check out our [**Getting Involved**](https://hubverse.io/community/#get-involved) page!
:::

::: notes
Thanks so much! I‚Äôm happy to chat about using hubverse in your domain, or collaborating on future forecasting infrastructure.
:::
